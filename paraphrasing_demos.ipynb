{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal \n",
    "- This notebook compares a few different paraphrasing and entity generation methods for covering variants when training LMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = \"Would you like some pizza?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some candidates \n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  #Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Would you like to eat pizza?',\n",
       " 'Do you like pizza?',\n",
       " 'Are you a fan of pizza?',\n",
       " 'Is there any pizza you would like?',\n",
       " 'Is there anything you would like to eat?',\n",
       " 'Would you like a meal?',\n",
       " 'Would you like to eat some pizza?',\n",
       " 'Would you like pizza?',\n",
       " 'Would you like some pizza?',\n",
       " 'Would you like some food?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = get_response(main_input, 10, 20)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Input_phrase:  Would you like some pizza?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('do you like pizza?', 21)\n",
      "('would you like to have some pizza?', 20)\n",
      "('would you like pizza?', 17)\n",
      "('would you like to have pizza?', 17)\n"
     ]
    }
   ],
   "source": [
    "# requires tweaking above works nice \n",
    "# Candidate generation with Parrot \n",
    "#     Adequacy (Is the meaning preserved adequately?)\n",
    "#     Fluency (Is the paraphrase fluent English?)\n",
    "#     Diversity (Lexical / Phrasal / Syntactical) (How much has the paraphrase changed the original sentence?)\n",
    "# Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.\n",
    "\n",
    "from parrot import Parrot\n",
    "import torch\n",
    "\n",
    "''' \n",
    "uncomment to get reproducable paraphrase generations\n",
    "def random_state(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "random_state(1234)\n",
    "'''\n",
    "\n",
    "#Init models (make sure you init ONLY once if you integrate this to your code)\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n",
    "\n",
    "phrases = [main_input]\n",
    "\n",
    "for phrase in phrases:\n",
    "  print(\"-\"*100)\n",
    "  print(\"Input_phrase: \", phrase)\n",
    "  print(\"-\"*100)\n",
    "  para_phrases = parrot.augment(input_phrase=phrase)\n",
    "  for para_phrase in para_phrases:\n",
    "   print(para_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8924, 0.8239, 0.7285, 0.8555, 0.4831, 0.6360, 0.9174, 0.9629, 1.0000,\n",
       "         0.7217]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANN Test \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences we want to encode. Example:\n",
    "input_embedding = model.encode(main_input, convert_to_tensor=True)\n",
    "candidate_embeddings = model.encode(candidates, convert_to_tensor=True)\n",
    "\n",
    "#semantic similarities  \n",
    "cosine_scores = util.cos_sim(input_embedding, candidate_embeddings)\n",
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like some pizza?\n",
      "tensor(0.8555) Is there any pizza you would like?\n",
      "tensor(0.8924) Would you like to eat pizza?\n",
      "tensor(0.9174) Would you like to eat some pizza?\n",
      "tensor(0.9629) Would you like pizza?\n",
      "tensor(1.0000) Would you like some pizza?\n"
     ]
    }
   ],
   "source": [
    "# Find N nearests neighbors \n",
    "print (main_input)\n",
    "n=5\n",
    "value, indices = cosine_scores.sort()\n",
    "for idx in indices[0][-n:]: \n",
    "    print (cosine_scores[0][idx], candidates[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some candidates\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#load models \n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "paratrf = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "sentrf = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Score:tensor(1.0000)\t\t\tWould you like some pizza?\n",
      " Score:tensor(0.9629)\t\t\tWould you like pizza?\n",
      " Score:tensor(0.9174)\t\t\tWould you like to eat some pizza?\n",
      " Score:tensor(0.8924)\t\t\tWould you like to eat pizza?\n",
      " Score:tensor(0.8555)\t\t\tIs there any pizza you would like?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def paraphrase(text):\n",
    "    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    def get_response(input_text, num_return_sequences, num_beams):\n",
    "        # Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization\n",
    "        batch = tokenizer(\n",
    "            [input_text],\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            max_length=60,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(torch_device)\n",
    "        translated = paratrf.generate(\n",
    "            **batch,\n",
    "            max_length=60,\n",
    "            num_beams=num_beams,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=1.5\n",
    "        )\n",
    "        tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        return tgt_text\n",
    "\n",
    "    # generate candidates\n",
    "    candidates = get_response(text, 10, 20)\n",
    "\n",
    "    # ranking model\n",
    "    # Sentences we want to encode. Example:\n",
    "    input_embedding = sentrf.encode(text, convert_to_tensor=True)\n",
    "    candidate_embeddings = sentrf.encode(candidates, convert_to_tensor=True)\n",
    "\n",
    "    # semantic similarities\n",
    "    cosine_scores = util.cos_sim(input_embedding, candidate_embeddings)\n",
    "    \n",
    "    # sort scores \n",
    "    value, indices = cosine_scores.topk(n)\n",
    "    output = \"\"\n",
    "    for idx in indices[0][-5:]:\n",
    "        output += \" Score:\" + str(cosine_scores[0][idx]) + \"\\t\\t\\t\" + candidates[idx] + \"\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "print(paraphrase(main_input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every possible recommendation, users can approve or reject \n",
    "# We want to take this into consideration and thers a few ways how\n",
    "#1. Create rejected emebeddings, avoid showing recs with semantic similarity to rejections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6flGYGGQZream5dnD5DowepmMqkBu at 0x1041c5680> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThis is indeed a test\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1675411158,\n",
       "  \"id\": \"cmpl-6flGYGGQZream5dnD5DowepmMqkBu\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 7,\n",
       "    \"prompt_tokens\": 6,\n",
       "    \"total_tokens\": 13\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best results are coming from OpenAI so let's use that \n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Load your API key from an environment variable or secret management service\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "response = openai.Completion.create(model=\"text-davinci-003\", prompt=\"Generate this is a test\", temperature=0.1, max_tokens=7)\n",
    "response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Flopsy, a white lop-eared rabbit who loves to eat carrots.\n",
      "2. Thumper, a brown and white spotted rabbit who loves to hop around the garden.\n",
      "3. Bun Bun, a grey and white Dutch rabbit who loves to cuddle.\n",
      "4. Cotton Tail, a white and brown Dutch rabbit who loves to explore.\n",
      "5. Snowball, an albino rabbit who loves to play in the snow.\n"
     ]
    }
   ],
   "source": [
    "#2 Entity Example Generation \n",
    "entity = \"rabbit\"\n",
    "n = 5 \n",
    "entity_prompt = f\"Create {n} examples of the following entity: \\n '{entity}'\" \n",
    "response = openai.Completion.create(model=\"text-davinci-003\", prompt=entity_prompt, temperature=0.25, max_tokens=100)\n",
    "print (response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "'I'm afraid I can't assist you with that! Would you like to order Chinese food instead?' \n",
      "\n",
      "'I'm sorry I can't be of more help! Would you like to get some tacos instead?'\n"
     ]
    }
   ],
   "source": [
    "#Response Variant Generation \n",
    "response = \"I'm not sure I can help you with that! Would you like to purchase pizza instead?\"\n",
    "variable = \"food\"\n",
    "\n",
    "\n",
    "n = 2\n",
    "if variable:\n",
    "    response_prompt = f\"create {n} sentences similar to:\\n '{response}' and keep {variable} in the new sentences.\" \n",
    "else: \n",
    "    response_prompt = f\"create {n} sentences similar to:\\n '{response}'\" \n",
    "    \n",
    "response = openai.Completion.create(model=\"text-davinci-003\", prompt=response_prompt, temperature=0.0, max_tokens=250)\n",
    "print (response['choices'][0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradio extension is already loaded. To reload it, use:\n",
      "  %reload_ext gradio\n"
     ]
    }
   ],
   "source": [
    "%load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. None of the students in the class passed the exam.\\n2. I have none of the answers to the questions.\\n3. She had none of the ingredients to make the cake.\\n4. None of the cars in the lot were for sale.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openAI request function for entities + paraphrases or both \n",
    "import openai \n",
    "import json \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def request_davinci(paraphrase, utterance, entity, n=1, temperature=0.25, max_tokens=100):\n",
    "    if paraphrase:\n",
    "        # 3 Response Variant Generation\n",
    "        if entity:\n",
    "            prompt = f\"create {n} sentences similar to:\\n '{utterance}' and keep {entity} in the new sentences.\"\n",
    "        else:\n",
    "            prompt = f\"create {n} sentences similar to:\\n '{utterance}'\"\n",
    "    else:\n",
    "        prompt = f\"Create {n} examples of the following: \\n '{entity}'\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "\n",
    "request_davinci(paraphrase=False, utterance=\"Hello, I would like to buy a coffee.\", entity=None, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/swan/lib/python3.9/site-packages/gradio/inputs.py:121: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/swan/lib/python3.9/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7883\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gradio demo \n",
    "# %%blocks \n",
    "import gradio as gr\n",
    "\n",
    "gr.Interface(request_davinci, \n",
    "[gr.inputs.Checkbox(default=False), \n",
    "gr.Text(value=None), \n",
    "gr.Text(value=None), \n",
    "gr.Slider(minimum=1, maximum=10, step=1), \n",
    "gr.Slider(minimum=0, maximum=1, step=0.1, value=0.2), \n",
    "gr.Slider(minimum=0, maximum=1000, value=500)],\n",
    "\"text\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/swan/lib/python3.9/site-packages/gradio/deprecation.py:43: UserWarning: You have unused kwarg parameters in Number, please remove them: {'placeholder': 1}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#OPEN AI \n",
    "# Understand options and how to create better prompts witht hem \n",
    "# Take into consideration of entity, try using entity in prompt name \n",
    "\n",
    "# GradIO options \n",
    "#1. Choose N return statements  \n",
    "\n",
    "#2. Choose generation type above 1-3 \n",
    "#3. Create a text \n",
    "#4. Generate N utterances  doone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gr.Markdown(\"# Paraphrase Generator\")\n",
    "\n",
    "\n",
    "generation_type = gr.Dropdown([\"similar sentences\", \"example entities\"])\n",
    "inp = gr.Textbox(placeholder=\"Enter something to paraphrase\")\n",
    "entity = gr.Textbox(placeholder=\"Enter any entities you want to see in the paraphrase\")\n",
    "create_n = gr.Number(label=\"n\") \n",
    "out = gr.Textbox()\n",
    "\n",
    "# inp.submit(fn=lambda x: \"hello\", \n",
    "#            inputs=inp, \n",
    "#            outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d5519b8dff683f2c3be1990911bc9ffcd12ced7c4bbab0c72f3ce3d70ba4131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
